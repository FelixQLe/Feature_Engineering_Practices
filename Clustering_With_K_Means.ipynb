{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b4c36ac",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "Unsupervised Learning Algorithms dont make use of a target, instead, their purpose is to learn some property of the data, to represent the structures of the features in a certain way. In the context of feature engineering for prediction, we could think of an unsupervised algorithm as a \"feature discovery\" technique.\n",
    "\n",
    "###### Clustering\n",
    "simply means the assigning of data points to groups based upon how similar the points are to each other. A clustering algorithm makes \"birds of a feather flock together,\" so to speak.\n",
    "\n",
    "When used for feature engineering, we could attempt to discover groups of customers representing a market segment, for instance, or geographic areas that share similar weather patterns. Adding a feature of cluster labels can help machine learning models untangle complicated relationships of space or proximity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1fa094",
   "metadata": {},
   "source": [
    "### Cluster Labels as a Feature\n",
    "Applied to a single real-valued feature, clustering acts like a traditional \"binning\" or \"discretization\" transform. On multiple features, it's \"multi-dimensional binning\" (sometimes called vector quantization).\n",
    "\n",
    "The motivating idea for adding cluster labels is that the clusters will break up complicated relationships across features into simpler chunks. OUr model can then just learn the simpler chunks one-by-one instead having to learn the complicated whole all at once. It's \"divide and conquer\" strategy.\n",
    "\n",
    "<img src=\"https://i.imgur.com/rraXFed.png\" width=\"800\" height=\"400\">\n",
    "\n",
    "The figure shows how clustering can improve a simle linear model. The curved relationship between the YearBuilt and salePrice is too complicated for this kind of model -- it underfits. On smaller chunks however the relationship is almostlinear, and that the model can learn easily.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe16b19",
   "metadata": {},
   "source": [
    "### K-Means Clustering\n",
    "There are a great many clustering algorithms. They differ primarily in how they measure \"similarity\" or \"Proximity\" and in what kinds of features they work with. The algorithm we'll use, k-means, is intuitive and easy to apply in feature engineering context. Depending on your application another algorithm might be more appropirate. \n",
    "\n",
    "K-mean clustering measures similarity using ordinary straight-line distance (Eculidean distance, in other words). It creates clusters by placing a number of points, called centroids, inside the feature-space. Each point in the dataset is assigned to the cluster of whichever centroid it's closest to. The 'K' in 'k-means' is how many centroids it creates. we define the k. \n",
    "\n",
    "we could imagine each centroid capturing points through a sequence of radiating circles. When sets of circles from competing centroids overlap they form a line. The result is what's called a Voronoi tessalation. The tessalation shows us to what clusters future data will be assigned; the tessalation is essentially what k-means learns from its training data.\n",
    "\n",
    "The clustering on the Ames housing dataset is k-means clustering. \n",
    "\n",
    "<img src=\"https://i.imgur.com/KSoLd3o.jpg.png\">\n",
    "\n",
    "#### Reviewing how k-means algorithm learns the clusters and what that means for feature engineering\n",
    "We focus on three parameters from scikit-learn;s implementation: n_clusters, max_iter, and n_init\n",
    "\n",
    "It's simple two-step process. The algorithm starts by randomly initializing some predefined number (n_clusters) of centroids. It then iterates over these two operations:\n",
    "\n",
    "1. assign points to the nearest cluster centroid\n",
    "2. move each centroid to minimize the distance to its points\n",
    "\n",
    "It iterates over these two steps until the centroids aren't moving anymore, or untill some maximum number of iterations has passed(max_iter).\n",
    "\n",
    "It often happens that the initial random position of the centroids ends in a poor clustering. For this reason the algorithm repeats a number of times(n_init) and returns the clustering that has the least total distance between each point and its centroid, the optimal clustering.\n",
    "\n",
    "The animuation below shows the algorithm in action. It illustrates the dependence of the result on the initial centroids and the importance of iterating until convergence.\n",
    "\n",
    "<img src=\"https://i.imgur.com/tBkCqXJ.gif\">\n",
    "\n",
    "We may need to increase the max_iter for a large number of clusters or n_init for a complex dataset. Ordinarily though the only parameter you'll need to choose your self is\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503c52cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python_3.11",
   "language": "python",
   "name": "python_3.11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
