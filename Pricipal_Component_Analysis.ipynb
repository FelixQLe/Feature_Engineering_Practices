{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ce5a63b",
   "metadata": {},
   "source": [
    "Principal Component Analysis (PCA), just like clustering is a partitioning of the dataset based on proximity, we could think of PCA as a partitioning of the variation in the data. PCA is a great tool to help us discover important relationships in the data and can also be used to create more informative features.\n",
    "\n",
    "* NOTE: PCA is typically applied to standardized data. With standardized data \"variation\" means \"correlation\". With unstandardized data \"variation\" means \"covariance\". "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b4efad4",
   "metadata": {},
   "source": [
    "### Principal Component Analysis\n",
    "\n",
    "Example with Abalone dataset, with Height and Diameter of their shells.\n",
    "\n",
    "<img src=\"https://i.imgur.com/rr8NCDy.png\">\n",
    "\n",
    "The shorter axis we might call the \"Size\" component: small height and small diameter (lower left) contrasted with large height and large diameter (upper right). The longer axis we might call the \"Shape\" component: small height and large diameter (flat shape) contrasted with large height and small diameter (round shape).\n",
    "\n",
    "Size and Shape are another way to describe the data(the shells), this is the idea of PCA, instead of describing the data with the original features, we describe it with its axes of variation. The axes of variation become the new features.\n",
    "\n",
    "<img src='https://i.imgur.com/XQlRD1q.png'>\n",
    "\n",
    "The new features PCA constructs are actually just linear combinations (weighted sums) of the original features:\n",
    "\n",
    "* df[\"Size\"] = 0.707 * X[\"Height\"] + 0.707 * X[\"Diameter\"]\n",
    "* df[\"Shape\"] = 0.707 * X[\"Height\"] - 0.707 * X[\"Diameter\"]\n",
    "\n",
    "There will be as many pricipal components as there are features in the original dataset. The Size componnent captures the majority of the variation between H and D. It's important to remember, the amount of variance in a component doesnt necessarily correspond to how good it is as predictor: It depends on what you're trying to predict. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac463d6",
   "metadata": {},
   "source": [
    "### PCA as Feature Engineering\n",
    "\n",
    "Two ways of using PCA as feature engineering\n",
    "\n",
    "* The first way is to use it as a descriptive technique. Since the components tell us about the variation, we could compute the MI scores for the components and see what kind of variation is most predictive of our target. That could give us ideas for kinds of features ot create -- a product of H and D if Size is important, or a ratio of H and D if Shape is important. we could even try clustering on one or more of the high-scoring components.\n",
    "\n",
    "\n",
    "* THe Second way is to use the components themselves as features. Because the components exposes the variational structure of the data directly, they can often be more informative than the original features. Here are some use-cases:\n",
    "\n",
    "     - Dimensionality reduction: When our features are highly redundant(multicolinear), PCA will parition out the redundancy into one or more ner-zero variance components, which we can the drop since they will contain little or no information.\n",
    " \n",
    "     - Anomaly detection: Unusual variation, not apparent from the original features, will often show up in the low-variance. These components could be highly informative in an anomaly or outlier detection task.\n",
    " \n",
    "     - Noise reduction: A collection of sensor readings will often share some common background noise. PCA can sometimes collect the (informative) signal into a smaller number of features while leaving the noise alone, thus boosting the singal-to-noise ratio.\n",
    " \n",
    "     - Decorrelation: Some ML algorithms struggle with highly-correlated features. PCA transforms correlated features into uncorrelated components, which could be easier for our algorithm to work with.\n",
    "\n",
    "\n",
    "PCA basically gives you direct access to the correlational structure of our data. You will no doubt come up with application of our own!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29fe0c05",
   "metadata": {},
   "source": [
    "#### PCA Best Practices\n",
    "There  are a few things to keep in mind when applying PCA:\n",
    "* PCA only works with numeric features, like continiuous quantities or counts\n",
    "* PCA is sensitive to scale. It's good practice to standardize our data before applying PCA, unless we know we have good reason not to.\n",
    "* Consider removing or constraining outliers, since they can have an undue influence on the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95815d56",
   "metadata": {},
   "source": [
    "### Working on Example - 1985 Automobiles\n",
    "In this example, we will apply PCA using it as a descriptive technique to discover features. we'll look at other use-cases in the excercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89310c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from IPython.display import display\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "\n",
    "#set up style \n",
    "plt.style.use('seaborn-whitegrid')\n",
    "plt.rc('figure', autolayout=True)\n",
    "plt.rc(\"axes\",\n",
    "      labelweight='bold',\n",
    "      labelsize='large',\n",
    "      titleweight='bold',\n",
    "      titlesize=14,\n",
    "      titlepad=10,)\n",
    "\n",
    "\n",
    "#defines functions plot_variance and make_mi_scores\n",
    "def plot_variance(pca, width=8, dpi=100):\n",
    "    #create figure\n",
    "    fig, axs = plt.subplots(1,2)\n",
    "    n = pca.n_components_\n",
    "    grid = np.arrange(1, n+1)\n",
    "    #Explained variance\n",
    "    evr = pca.explained_variance_ratio_\n",
    "    axs[0].bar(grid, evr)\n",
    "    axs[0].set(xlabel='Component', title=\"% Cumulative Variance\", ylim=(0.0, 1.0))\n",
    "    #set up figure\n",
    "    fig.set(figwidth=8, dpi=100)\n",
    "    return axs\n",
    "\n",
    "def make_mi_score(X, y, discrete_features):\n",
    "    mi_scores = mutual_info_regression(X, y, discreate_featues=discrete_features)\n",
    "    mi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=X.columns)\n",
    "    mi_scores = mi_scores.sort_values(ascending=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pro_project",
   "language": "python",
   "name": "pro_project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
