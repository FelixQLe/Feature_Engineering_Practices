{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "311224b2",
   "metadata": {},
   "source": [
    "Principal Component Analysis (PCA), just like clustering is a partitioning of the dataset based on proximity, we could think of PCA as a partitioning of the variation in the data. PCA is a great tool to help us discover important relationships in the data and can also be used to create more informative features.\n",
    "\n",
    "* NOTE: PCA is typically applied to standardized data. With standardized data \"variation\" means \"correlation\". With unstandardized data \"variation\" means \"covariance\". "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cbb35ae",
   "metadata": {},
   "source": [
    "### Principal Component Analysis\n",
    "\n",
    "Example with Abalone dataset, with Height and Diameter of their shells.\n",
    "\n",
    "<img src=\"https://i.imgur.com/rr8NCDy.png\">\n",
    "\n",
    "The shorter axis we might call the \"Size\" component: small height and small diameter (lower left) contrasted with large height and large diameter (upper right). The longer axis we might call the \"Shape\" component: small height and large diameter (flat shape) contrasted with large height and small diameter (round shape).\n",
    "\n",
    "Size and Shape are another way to describe the data(the shells), this is the idea of PCA, instead of describing the data with the original features, we describe it with its axes of variation. The axes of variation become the new features.\n",
    "\n",
    "<img src='https://i.imgur.com/XQlRD1q.png'>\n",
    "\n",
    "The new features PCA constructs are actually just linear combinations (weighted sums) of the original features:\n",
    "\n",
    "* df[\"Size\"] = 0.707 * X[\"Height\"] + 0.707 * X[\"Diameter\"]\n",
    "* df[\"Shape\"] = 0.707 * X[\"Height\"] - 0.707 * X[\"Diameter\"]\n",
    "\n",
    "There will be as many pricipal components as there are features in the original dataset. The Size componnent captures the majority of the variation between H and D. It's important to remember, the amount of variance in a component doesnt necessarily correspond to how good it is as predictor: It depends on what you're trying to predict. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e82ea85",
   "metadata": {},
   "source": [
    "### PCA as Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc5e19b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pro_project",
   "language": "python",
   "name": "pro_project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
